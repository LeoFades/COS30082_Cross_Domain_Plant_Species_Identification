# -*- coding: utf-8 -*-
"""hftl-config-changed-4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12zcurJfr0WY9Wy7swBo8WdAkKe5KSxSg
"""

# ----------------------------
# Imports
# ----------------------------
import os, math, random, json
from pathlib import Path
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, TensorDataset
from torchvision import transforms
from sklearn.model_selection import StratifiedShuffleSplit
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

"""I try making Herbarium-Field Triplet Loss approach. This is a two stream approach into a triplet loss.

"""

# ----------------------------
# Paths (update if needed)
# ----------------------------
SAVE_DIR = "/kaggle/working/HFTL/"
os.makedirs(SAVE_DIR, exist_ok=True)

DATASET_ROOT = "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset"
LIST_DIR = os.path.join(DATASET_ROOT, "list")  # contains annotation files
TRAIN_ROOT = os.path.join(DATASET_ROOT, "train")  # contains herbarium/ and photo/
TEST_ROOT = os.path.join(DATASET_ROOT, "test")


EMB_SAVE_DIR = "/kaggle/working/embeddings"
os.makedirs(EMB_SAVE_DIR, exist_ok=True)

# Configs
IMAGE_SIZE = 518
BATCH_SIZE_EXTRACT = 32
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
SEED = 42

# ----------------------------
# Utilities
# ----------------------------
def seed_everything(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

seed_everything(SEED)

def accuracy_simple(pos_dist, neg_dist):
    return (pos_dist < neg_dist).float().mean().item()

# ----------------------------
# NEW: Species mapping and dataset loading
# ----------------------------
def load_species_mapping(species_list_path):
    """
    Load species mapping from species_list.txt
    Returns: dict {species_id: species_name}
    """
    species_map = {}
    with open(species_list_path, 'r') as f:
        for line in f:
            parts = line.strip().split('; ')
            if len(parts) >= 2:
                species_id = int(parts[0])
                species_name = '; '.join(parts[1:])
                species_map[species_id] = species_name
    print(f"Loaded {len(species_map)} species from mapping file")
    return species_map

def load_annotation_file(annotation_path, dataset_root):
    """
    Load annotation file and return list of (image_path, species_id)
    """
    samples = []
    with open(annotation_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                img_rel_path = parts[0]
                species_id = int(parts[1])
                # Construct full path
                img_full_path = os.path.join(dataset_root, img_rel_path)
                if os.path.exists(img_full_path):
                    samples.append((img_full_path, species_id))
                else:
                    print(f"Warning: Image not found: {img_full_path}")
    print(f"Loaded {len(samples)} samples from {annotation_path}")
    return samples

class AnnotationDataset(Dataset):
    """
    Dataset that reads images from annotation files with proper species mapping
    """
    def __init__(self, annotation_path, dataset_root, transform=None, species_map=None,filter_subfolder=None):
        self.dataset_root = Path(dataset_root)
        self.transform = transform

        # Load samples from annotation file
        samples = load_annotation_file(annotation_path, dataset_root)

        if filter_subfolder is not None:
            samples = [(path, sid) for path, sid in samples if f"/{filter_subfolder}/" in path]

        self.samples= samples
        # Create label mapping (species_id -> 0-based index)
        all_species_ids = list(set([species_id for _, species_id in self.samples]))
        self.species_to_idx = {species_id: idx for idx, species_id in enumerate(sorted(all_species_ids))}
        self.idx_to_species = {idx: species_id for species_id, idx in self.species_to_idx.items()}

        # Apply mapping to samples
        self.samples = [(img_path, self.species_to_idx[species_id])
                       for img_path, species_id in self.samples]

        # Build class_to_indices for triplet sampling
        self.class_to_indices = {}
        for idx, (_, label) in enumerate(self.samples):
            self.class_to_indices.setdefault(label, []).append(idx)

        print(f"Dataset: {len(self.samples)} images, {len(self.class_to_indices)} classes")

        # Store species map for reference
        self.species_map = species_map

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = Image.open(path).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return img, label

# ----------------------------
# Transforms (unchanged)
# ----------------------------
def get_transforms(image_size):
    train_transform = transforms.Compose([
        transforms.Resize(int(image_size * 1.14)),
        transforms.CenterCrop(image_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
    ])
    return train_transform


#------------Adding Data Augmentation-------------------#
def get_train_transforms(image_size):
    # train_transform = transforms.Compose([
    #     transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)),
    #     transforms.Resize(int(image_size * 1.14)),
    #     transforms.CenterCrop(image_size),
    #     transforms.RandomHorizontalFlip(),
    #     #transforms.RandomVerticalFlip(),
    #     transforms.RandomRotation(10),  # degrees
    #     transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.02),
    #     #transforms.RandomGrayscale(p=0.1),
    #     transforms.ToTensor(),
    #     transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
    # ])
    train_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(0.2, 0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])
    return train_transform

# ----------------------------
# DINOv2 loader (unchanged)
# ----------------------------
def load_dinov2_from_kagglehub(device):
    import kagglehub, timm, os
    print("\n" + "="*60)
    print("Loading DINOv2 Feature Extractor from KaggleHub...")
    print("="*60)
    dinov2_path = kagglehub.model_download("juliostat/dinov2_patch14_reg4_onlyclassifier_then_all/PyTorch/default")
    ckpt_file = None
    for root, dirs, files in os.walk(dinov2_path):
        for f in files:
            if f.endswith(".pth.tar"):
                ckpt_file = os.path.join(root, f)
                break
        if ckpt_file:
            break
    if not ckpt_file:
        raise FileNotFoundError("No .pth.tar checkpoint found.")
    print("Found checkpoint:", ckpt_file)
    checkpoint = torch.load(ckpt_file, map_location="cpu", weights_only=False)
    arch_name = checkpoint.get("arch", "vit_base_patch14_reg4_dinov2.lvd142m")
    print("Arch:", arch_name)
    try:
        dinov2_model = timm.create_model(arch_name, pretrained=False)
    except Exception:
        dinov2_model = timm.create_model("vit_base_patch14_reg4_dinov2.lvd142m", pretrained=False)
    state_dict = checkpoint.get("state_dict_ema", checkpoint.get("state_dict"))
    dinov2_model.load_state_dict(state_dict, strict=False)
    dinov2_model.eval()
    dinov2_model.to(device)
    print("DINOv2 loaded to", device)
    return dinov2_model

# ----------------------------
# UPDATED: Embedding extraction for annotation-based datasets
# ----------------------------
def extract_and_save_embeddings(dinov2, annotation_path, dataset_root, save_prefix,filter_subfolder,
                                species_map=None, image_size=IMAGE_SIZE,
                                batch_size=BATCH_SIZE_EXTRACT, device=DEVICE):
    """
    Extract embeddings using annotation files
    """
    transform = get_train_transforms(image_size)
    dataset = AnnotationDataset(annotation_path, dataset_root, transform=transform, species_map=species_map,filter_subfolder=filter_subfolder)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

    all_embs = []
    all_labels = []
    all_species_ids = []  # Store original species IDs

    dinov2.eval()
    with torch.no_grad():
        for imgs, labels in tqdm(loader, desc=f"Embedding {save_prefix}"):
            imgs = imgs.to(device)
            emb = dinov2(imgs)
            if isinstance(emb, (tuple, list)):
                emb = emb[0]
            emb = torch.flatten(emb, 1)
            all_embs.append(emb.cpu())
            all_labels.append(labels)

            # Convert back to original species IDs
            batch_species_ids = [dataset.idx_to_species[label.item()] for label in labels]
            all_species_ids.extend(batch_species_ids)

    all_embs = torch.cat(all_embs, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    all_species_ids = torch.tensor(all_species_ids, dtype=torch.long)

    # Save
    emb_path = os.path.join(EMB_SAVE_DIR, f"{save_prefix}_embeddings.pt")
    lbl_path = os.path.join(EMB_SAVE_DIR, f"{save_prefix}_labels.pt")
    species_path = os.path.join(EMB_SAVE_DIR, f"{save_prefix}_species_ids.pt")
    meta_path = os.path.join(EMB_SAVE_DIR, f"{save_prefix}_meta.json")

    torch.save(all_embs, emb_path)
    torch.save(all_labels, lbl_path)
    torch.save(all_species_ids, species_path)

    # Save meta information
    meta = {
        "num_samples": int(all_embs.shape[0]),
        "embedding_dim": int(all_embs.shape[1]),
        "num_classes": len(dataset.class_to_indices),
        "annotation_file": annotation_path,
        "dataset_root": dataset_root,
        "species_mapping": {str(k): v for k, v in dataset.species_to_idx.items()}
    }
    with open(meta_path, "w") as f:
        json.dump(meta, f, indent=2)

    print(f"Saved embeddings -> {emb_path}")
    return all_embs, all_labels, all_species_ids, meta

def load_embeddings_from_drive(prefix):
    emb_path = os.path.join(EMB_SAVE_DIR, f"{prefix}_embeddings.pt")
    lbl_path = os.path.join(EMB_SAVE_DIR, f"{prefix}_labels.pt")
    species_path = os.path.join(EMB_SAVE_DIR, f"{prefix}_species_ids.pt")
    meta_path = os.path.join(EMB_SAVE_DIR, f"{prefix}_meta.json")

    if not all(os.path.exists(p) for p in [emb_path, lbl_path, species_path, meta_path]):
        return None

    embs = torch.load(emb_path, map_location="cpu")
    labels = torch.load(lbl_path, map_location="cpu")
    species_ids = torch.load(species_path, map_location="cpu")
    with open(meta_path, "r") as f:
        meta = json.load(f)

    print(f"Loaded {prefix} embeddings from {emb_path} (shape={embs.shape})")
    return embs, labels, species_ids, meta

# ----------------------------
# UPDATED: Build class-index mapping using species IDs
# ----------------------------
def build_class_index_map(species_ids):
    """
    species_ids: 1D tensor of original species IDs (N,)
    returns dict {species_id: [indices,...]}
    """
    class_to_indices = {}
    for i, species_id in enumerate(species_ids.tolist()):
        class_to_indices.setdefault(int(species_id), []).append(i)
    return class_to_indices

# ----------------------------
# Projection head (unchanged)
# ----------------------------
# class SharedProjector(nn.Module):
#     def __init__(self, input_dim, proj_dim=256):
#         super().__init__()
#         self.net = nn.Sequential(
#             nn.Linear(input_dim, 1024),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm1d(1024),
#             nn.Linear(1024, proj_dim)
#         )

#     def forward(self, x):
#         return self.net(x)

# ----------------------------
# Projection head (changed)
# ----------------------------
import torch.nn.functional as F
class SharedProjector(nn.Module):
    def __init__(self, input_dim, proj_dim=512, hidden_dim=1024, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(p=dropout),
            nn.Linear(hidden_dim, proj_dim)
        )

    def forward(self, x):
        out = self.net(x)
        return F.normalize(out, dim=1)

# Add this before the main training to verify the species IDs are loaded correctly
def verify_species_ids():
    photo_loaded = load_embeddings_from_drive("photo")
    herb_loaded = load_embeddings_from_drive("herbarium")

    if photo_loaded and herb_loaded:
        photo_embs, photo_labels, photo_species_ids, meta_photo = photo_loaded
        herb_embs, herb_labels, herb_species_ids, meta_herb = herb_loaded

        print("=== SPECIES ID VERIFICATION ===")
        print(f"Photo species IDs dtype: {photo_species_ids.dtype}")
        print(f"Herb species IDs dtype: {herb_species_ids.dtype}")
        print(f"Photo species IDs min/max: {photo_species_ids.min()}/{photo_species_ids.max()}")
        print(f"Herb species IDs min/max: {herb_species_ids.min()}/{herb_species_ids.max()}")

        # Check if they're the same type
        photo_unique = set(photo_species_ids.tolist())
        herb_unique = set(herb_species_ids.tolist())
        common = photo_unique.intersection(herb_unique)

        print(f"Photo unique: {len(photo_unique)}, Herb unique: {len(herb_unique)}, Common: {len(common)}")
        print(f"Sample common species: {list(common)[:5]}")

        return len(common) > 0
    return False

"""# AFTER TRIPLET LOSS - GOT HEAD

"""

def evaluate_model(projector, photo_embs, photo_species_ids, herb_embs, herb_species_ids, k=5):
    """Evaluate retrieval performance using k-NN and compute average per-class accuracy"""
    projector.eval()

    with torch.no_grad():
        # Project all embeddings
        photo_proj = projector(photo_embs)
        herb_proj = projector(herb_embs)

        # Normalize for cosine similarity
        photo_proj = nn.functional.normalize(photo_proj, p=2, dim=1)
        herb_proj = nn.functional.normalize(herb_proj, p=2, dim=1)

        # Compute similarity matrix
        similarity = torch.mm(photo_proj, herb_proj.t())  # (n_photos, n_herb)

        # Get top-k predictions
        topk_similarities, topk_indices = similarity.topk(k=k, dim=1)

        # Convert herbarium indices to species IDs
        topk_species = herb_species_ids[topk_indices]

        # -----------------------------
        # Overall accuracy@k
        # -----------------------------
        correct = 0
        for i in range(len(photo_species_ids)):
            target_species = photo_species_ids[i]
            if target_species in topk_species[i]:
                correct += 1

        accuracy = correct / len(photo_species_ids)

        # -----------------------------
        # Mean Average Precision (mAP)
        # -----------------------------
        avg_precision = 0.0
        for i in range(len(photo_species_ids)):
            target_species = photo_species_ids[i]
            precision_at_k = 0.0
            num_relevant = 0

            for j in range(k):
                if topk_species[i][j] == target_species:
                    num_relevant += 1
                    precision_at_k += num_relevant / (j + 1)

            if num_relevant > 0:
                avg_precision += precision_at_k / num_relevant

        map_score = avg_precision / len(photo_species_ids)

        # -----------------------------
        # Average per-class accuracy
        # -----------------------------
        class_to_indices = {}
        for idx, species_id in enumerate(photo_species_ids.tolist()):
            class_to_indices.setdefault(species_id, []).append(idx)

        per_class_acc = {}
        for species_id, indices in class_to_indices.items():
            correct_class = sum(
                1 for i in indices if species_id in topk_species[i]
            )
            per_class_acc[species_id] = correct_class / len(indices)

        avg_per_class_acc = sum(per_class_acc.values()) / len(per_class_acc)

    return accuracy, map_score, avg_per_class_acc, per_class_acc

def plot_training_history(history, save_path=None):
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

    # Loss
    ax1.plot(history['train_loss'], label='Training Loss')
    ax1.set_title('Triplet Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)

    # Accuracy
    ax2.plot(history['train_acc'], label='Train Triplet Acc', linestyle='--')
    ax2.plot(history['val_acc@1'], label='Val Acc@1', linewidth=2)
    ax2.plot(history['val_acc@5'], label='Val Acc@5', linewidth=2)
    ax2.set_title('Retrieval Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True)

    # mAP
    ax3.plot(history['val_map'], label='Val mAP', color='red', linewidth=2)
    ax3.set_title('Mean Average Precision (mAP)')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('mAP')
    ax3.legend()
    ax3.grid(True)

    # Learning Rate
    ax4.plot(history['learning_rate'], label='Learning Rate', color='purple')
    ax4.set_title('Learning Rate Schedule')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Learning Rate')
    ax4.legend()
    ax4.grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Training plots saved to {save_path}")

    plt.show()

    # Print final results
    print("\n=== FINAL RESULTS ===")
    print(f"Best Validation mAP: {max(history['val_map']):.4f}")
    print(f"Best Validation Acc@1: {max(history['val_acc@1']):.4f}")
    print(f"Best Validation Acc@5: {max(history['val_acc@5']):.4f}")
    print(f"Final Training Loss: {history['train_loss'][-1]:.4f}")

def load_class_lists():
    """Load classes with and without pairs"""
    with_pairs_path = os.path.join(LIST_DIR, "class_with_pairs.txt")
    without_pairs_path = os.path.join(LIST_DIR, "class_without_pairs.txt")

    with_pairs = set()
    without_pairs = set()

    # Load classes with pairs
    with open(with_pairs_path, 'r') as f:
        for line in f:
            class_id = int(line.strip())
            with_pairs.add(class_id)

    # Load classes without pairs
    with open(without_pairs_path, 'r') as f:
        for line in f:
            class_id = int(line.strip())
            without_pairs.add(class_id)

    print(f"Loaded {len(with_pairs)} classes with pairs")
    print(f"Loaded {len(without_pairs)} classes without pairs")

    return with_pairs, without_pairs

def evaluate_by_class_type(projector, photo_embs, photo_species_ids, herb_embs, herb_species_ids,
                           with_pairs_classes, without_pairs_classes, k=5):
    """Evaluate separately for classes with pairs and without pairs, including per-species accuracy"""
    projector.eval()

    with torch.no_grad():
        # Project all embeddings
        photo_proj = projector(photo_embs)
        herb_proj = projector(herb_embs)

        # Normalize for cosine similarity
        photo_proj = nn.functional.normalize(photo_proj, p=2, dim=1)
        herb_proj = nn.functional.normalize(herb_proj, p=2, dim=1)

        # Compute similarity matrix
        similarity = torch.mm(photo_proj, herb_proj.t())

        # Get top-k predictions
        topk_similarities, topk_indices = similarity.topk(k=k, dim=1)
        topk_species = herb_species_ids[topk_indices]

        # Separate indices by class type
        with_pairs_indices = []
        without_pairs_indices = []

        for i in range(len(photo_species_ids)):
            species_id = photo_species_ids[i].item()
            if species_id in with_pairs_classes:
                with_pairs_indices.append(i)
            elif species_id in without_pairs_classes:
                without_pairs_indices.append(i)

        print(f"Evaluation samples - With pairs: {len(with_pairs_indices)}, Without pairs: {len(without_pairs_indices)}")

        # -----------------------------
        # Evaluate classes with pairs
        # -----------------------------
        with_pairs_acc = 0.0
        with_pairs_map = 0.0
        with_pairs_per_species_acc = {}

        if with_pairs_indices:
            with_pairs_acc, with_pairs_map = calculate_metrics(
                with_pairs_indices, photo_species_ids, topk_species, k
            )

            # Compute per-species accuracy
            for species_id in with_pairs_classes:
                species_indices = [i for i in with_pairs_indices if photo_species_ids[i].item() == species_id]
                if species_indices:
                    species_acc, _ = calculate_metrics(species_indices, photo_species_ids, topk_species, k)
                    with_pairs_per_species_acc[species_id] = species_acc
            # Average per-species accuracy
            with_pairs_avg_per_species_acc = sum(with_pairs_per_species_acc.values()) / len(with_pairs_per_species_acc)
        else:
            with_pairs_avg_per_species_acc = 0.0

        # -----------------------------
        # Evaluate classes without pairs
        # -----------------------------
        without_pairs_acc = 0.0
        without_pairs_map = 0.0
        without_pairs_per_species_acc = {}

        if without_pairs_indices:
            without_pairs_acc, without_pairs_map = calculate_metrics(
                without_pairs_indices, photo_species_ids, topk_species, k
            )

            # Compute per-species accuracy
            for species_id in without_pairs_classes:
                species_indices = [i for i in without_pairs_indices if photo_species_ids[i].item() == species_id]
                if species_indices:
                    species_acc, _ = calculate_metrics(species_indices, photo_species_ids, topk_species, k)
                    without_pairs_per_species_acc[species_id] = species_acc
            # Average per-species accuracy
            without_pairs_avg_per_species_acc = sum(without_pairs_per_species_acc.values()) / len(without_pairs_per_species_acc)
        else:
            without_pairs_avg_per_species_acc = 0.0

        return {
            'with_pairs': {
                'acc@k': with_pairs_acc,
                'map': with_pairs_map,
                'count': len(with_pairs_indices),
                'avg_per_species_acc': with_pairs_avg_per_species_acc,
                'per_species_acc': with_pairs_per_species_acc
            },
            'without_pairs': {
                'acc@k': without_pairs_acc,
                'map': without_pairs_map,
                'count': len(without_pairs_indices),
                'avg_per_species_acc': without_pairs_avg_per_species_acc,
                'per_species_acc': without_pairs_per_species_acc
            }
        }


def calculate_metrics(indices, photo_species_ids, topk_species, k):
    """Calculate accuracy and mAP for given indices"""
    correct = 0
    avg_precision = 0.0

    for i in indices:
        target_species = photo_species_ids[i]

        # Accuracy
        if target_species in topk_species[i]:
            correct += 1

        # mAP
        precision_at_k = 0.0
        num_relevant = 0

        for j in range(k):
            if topk_species[i][j] == target_species:
                num_relevant += 1
                precision_at_k += num_relevant / (j + 1)

        if num_relevant > 0:
            avg_precision += precision_at_k / num_relevant

    accuracy = correct / len(indices) if indices else 0.0
    map_score = avg_precision / len(indices) if indices else 0.0

    return accuracy, map_score

def plot_class_comparison(history, save_path=None):
    """Plot comparison between classes with and without pairs"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

    # Overall accuracy comparison
    ax1.plot(history['val_acc@1'], label='Overall Acc@1', linewidth=2)
    ax1.plot(history['with_pairs_acc@1'], label='With Pairs Acc@1', linestyle='--')
    ax1.plot(history['without_pairs_acc@1'], label='Without Pairs Acc@1', linestyle='--')
    ax1.set_title('Accuracy@1 Comparison')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True)

    # Accuracy@5 comparison
    ax2.plot(history['val_acc@5'], label='Overall Acc@5', linewidth=2)
    ax2.plot(history['with_pairs_acc@5'], label='With Pairs Acc@5', linestyle='--')
    ax2.plot(history['without_pairs_acc@5'], label='Without Pairs Acc@5', linestyle='--')
    ax2.set_title('Accuracy@5 Comparison')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True)

    # mAP comparison
    ax3.plot(history['val_map'], label='Overall mAP', linewidth=2)
    ax3.plot(history['with_pairs_map'], label='With Pairs mAP', linestyle='--')
    ax3.plot(history['without_pairs_map'], label='Without Pairs mAP', linestyle='--')
    ax3.set_title('mAP Comparison')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('mAP')
    ax3.legend()
    ax3.grid(True)

    # Training metrics
    ax4.plot(history['train_loss'], label='Training Loss')
    ax4.plot(history['train_acc'], label='Training Triplet Acc')
    ax4.set_title('Training Progress')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Metric Value')
    ax4.legend()
    ax4.grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Class comparison plot saved to {save_path}")

    plt.show()

    # Print final comparison
    print("\n=== FINAL CLASS-TYPE COMPARISON ===")
    print(f"Overall - Acc@1: {history['val_acc@1'][-1]:.4f}, Acc@5: {history['val_acc@5'][-1]:.4f}, mAP: {history['val_map'][-1]:.4f}")
    print(f"With Pairs - Acc@1: {history['with_pairs_acc@1'][-1]:.4f}, Acc@5: {history['with_pairs_acc@5'][-1]:.4f}, mAP: {history['with_pairs_map'][-1]:.4f}")
    print(f"Without Pairs - Acc@1: {history['without_pairs_acc@1'][-1]:.4f}, Acc@5: {history['without_pairs_acc@5'][-1]:.4f}, mAP: {history['without_pairs_map'][-1]:.4f}")

"""# Using test set as validation set"""

def triplet_generator_semihard_two_domain(
        photo_class_to_indices,
        herb_class_to_indices,
        photo_embs,
        herb_embs,
        batch_size,
        margin=0.2):

    """Semi-hard triplet sampler where anchor can be field OR herbarium.
       Anchor and positive must be from different domains but same species."""

    all_species = list(herb_class_to_indices.keys())

    # Normalized embeddings (optional)
    photo_norm = photo_embs
    herb_norm = herb_embs

    # ------- Build anchor pools -------
    # Anchors must be species that exist in BOTH domains
    valid_species = [
        s for s in all_species
        if s in photo_class_to_indices and len(photo_class_to_indices[s]) > 0
    ]

    anchor_pool = []
    for species in valid_species:
        # Add all field images of this species
        for idx in photo_class_to_indices[species]:
            anchor_pool.append(("photo", idx, species))

        # Add all herb images of this species
        for idx in herb_class_to_indices[species]:
            anchor_pool.append(("herb", idx, species))

    while True:
        random.shuffle(anchor_pool)
        batch_triplets = []

        for anchor_domain, anchor_idx, anchor_species in anchor_pool:

            # ---------- Select positive in the OTHER DOMAIN ----------
            if anchor_domain == "photo":
                pos_candidates = herb_class_to_indices[anchor_species]
                anc_vec = photo_norm[anchor_idx]
                pos_domain = "herb"
            else:
                pos_candidates = photo_class_to_indices[anchor_species]
                anc_vec = herb_norm[anchor_idx]
                pos_domain = "photo"

            pos_idx = random.choice(pos_candidates)
            pos_vec = (herb_norm if pos_domain == "herb" else photo_norm)[pos_idx]

            pos_dist = torch.norm(anc_vec - pos_vec).item()

            # ---------- Semi-hard negative ----------
            neg_idx = None
            attempts = 0
            max_attempts = 50

            while attempts < max_attempts:
                attempts += 1

                neg_species = random.choice([s for s in valid_species if s != anchor_species])

                # negative can be from EITHER domain
                if random.random() < 0.5:
                    neg_candidates = herb_class_to_indices[neg_species]
                    neg_domain = "herb"
                    neg_vec_pool = herb_norm
                else:
                    if neg_species not in photo_class_to_indices:
                        continue
                    neg_candidates = photo_class_to_indices[neg_species]
                    neg_domain = "photo"
                    neg_vec_pool = photo_norm

                neg_candidate = random.choice(neg_candidates)
                neg_vec = neg_vec_pool[neg_candidate]
                neg_dist = torch.norm(anc_vec - neg_vec).item()

                if pos_dist < neg_dist < pos_dist + margin:
                    neg_idx = (neg_domain, neg_candidate)
                    break

            # fallback random negative
            if neg_idx is None:
                neg_species = random.choice([s for s in valid_species if s != anchor_species])
                if random.random() < 0.5:
                    neg_idx = ("herb", random.choice(herb_class_to_indices[neg_species]))
                else:
                    neg_idx = ("photo", random.choice(photo_class_to_indices[neg_species]))

            # Store triplet with domain info
            batch_triplets.append((anchor_domain, anchor_idx,
                                   pos_domain, pos_idx,
                                   neg_idx[0], neg_idx[1]))

            if len(batch_triplets) >= batch_size:
                yield batch_triplets
                batch_triplets = []

def train_triplet_with_class_evaluation(
        photo_embs, photo_species_ids,
        herb_embs, herb_species_ids,
        test_embs, test_species_ids,
        dinov2,
        proj_dim=256,
        epochs=20,
        batch_size=32,
        margin=0.2,
        lr=3e-4,
        device=DEVICE):

    print("Starting training with class-type evaluation...")

    # ----------------------------------------------------
    # Load class lists
    # ----------------------------------------------------
    with_pairs_classes, without_pairs_classes = load_class_lists()

    input_dim = photo_embs.shape[1]
    projector = SharedProjector(input_dim, proj_dim=proj_dim).to(device)

    optimizer = torch.optim.AdamW(projector.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    criterion = nn.TripletMarginLoss(margin=margin, p=2)

    herb_class_to_indices = build_class_index_map(herb_species_ids)
    photo_class_to_indices = build_class_index_map(photo_species_ids)

    # ----------------------------------------------------
    # Compute valid species that have pairs in both domains
    # ----------------------------------------------------
    valid_species = [
        s for s in herb_class_to_indices.keys()
        if s in photo_class_to_indices
        and len(photo_class_to_indices[s]) > 0
        and len(herb_class_to_indices[s]) > 0
    ]

    total_anchors = 0
    for s in valid_species:
        total_anchors += len(photo_class_to_indices[s])
        total_anchors += len(herb_class_to_indices[s])

    print(f"Training species with pairs: {len(valid_species)}; "
          f"total anchors: {total_anchors}")

    # ----------------------------------------------------
    # History container
    # ----------------------------------------------------
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_acc@1': [],
        'val_acc@5': [],
        'avg_per_class_acc':[],
        'val_map': [],
        'with_pairs_acc@1': [],
        'with_pairs_acc@5': [],
        'with_pairs_avg_per_class_acc':[],
        'with_pairs_map': [],
        'without_pairs_acc@1': [],
        'without_pairs_acc@5': [],
        'without_pairs_avg_per_class_acc':[],
        'without_pairs_map': [],
        'learning_rate': []
    }

    best_map = 0.0
    patience = 5
    best_epoch = 0
    early_stop_counter = 0
    best_state_dict = None

    # Ensure tensors
    if not torch.is_tensor(photo_embs):
        photo_embs = torch.tensor(photo_embs)
    if not torch.is_tensor(herb_embs):
        herb_embs = torch.tensor(herb_embs)
    if not torch.is_tensor(test_embs):
        test_embs = torch.tensor(test_embs)

    # ----------------------------------------------------
    # Training loop
    # ----------------------------------------------------
    for epoch in range(1, epochs + 1):

        projector.train()
        running_loss, running_acc, n_batches = 0.0, 0.0, 0

        triplet_gen = triplet_generator_semihard_two_domain(
            photo_class_to_indices,
            herb_class_to_indices,
            photo_embs,
            herb_embs,
            batch_size,
            margin=margin
        )

        steps_per_epoch = max(total_anchors // batch_size, 1)

        # ------------------------------
        # Batches
        # ------------------------------
        for _ in tqdm(range(steps_per_epoch), desc=f"Epoch {epoch}"):

            batch_triplets = next(triplet_gen)
            # list of (ad, ai, pd, pi, nd, ni)

            anc_dom, anchor_idx_list = [], []
            pos_dom, pos_idx_list = [], []
            neg_dom, neg_idx_list = [], []

            for (ad, ai, pd, pi, nd, ni) in batch_triplets:
                anc_dom.append(ad)
                anchor_idx_list.append(int(ai))
                pos_dom.append(pd)
                pos_idx_list.append(int(pi))
                neg_dom.append(nd)
                neg_idx_list.append(int(ni))

            # ------------------------------
            # Build anc, pos, neg tensors
            # ------------------------------
            anc_list = []
            for d, i in zip(anc_dom, anchor_idx_list):
                anc_list.append(
                    (photo_embs if d == "photo" else herb_embs)[i].to(device)
                )
            anc = torch.stack(anc_list)

            pos_list = []
            for d, i in zip(pos_dom, pos_idx_list):
                pos_list.append(
                    (photo_embs if d == "photo" else herb_embs)[i].to(device)
                )
            pos = torch.stack(pos_list)

            neg_list = []
            for d, i in zip(neg_dom, neg_idx_list):
                neg_list.append(
                    (photo_embs if d == "photo" else herb_embs)[i].to(device)
                )
            neg = torch.stack(neg_list)

            # ------------------------------
            # Forward
            # ------------------------------
            anc_z = F.normalize(projector(anc), dim=1)
            pos_z = F.normalize(projector(pos), dim=1)
            neg_z = F.normalize(projector(neg), dim=1)

            loss = criterion(anc_z, pos_z, neg_z)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            pos_dist = torch.norm(anc_z - pos_z, p=2, dim=1)
            neg_dist = torch.norm(anc_z - neg_z, p=2, dim=1)
            batch_acc = (pos_dist < neg_dist).float().mean().item()

            running_loss += loss.item()
            running_acc += batch_acc
            n_batches += 1

        # ----------------------------------------------------
        # Validation
        # ----------------------------------------------------
        projector.eval()

        test_embs_dev = test_embs.to(device)
        herb_embs_dev = herb_embs.to(device)

        val_acc1, val_map,avg_per_class_acc, per_class_acc = evaluate_model(
            projector,
            test_embs_dev,
            test_species_ids.to(device),
            herb_embs_dev,
            herb_species_ids.to(device),
            k=1
        )

        val_acc5, _,_ , _ = evaluate_model(
            projector,
            test_embs_dev,
            test_species_ids.to(device),
            herb_embs_dev,
            herb_species_ids.to(device),
            k=5
        )

        class_results_1 = evaluate_by_class_type(
            projector,
            test_embs_dev, test_species_ids.to(device),
            herb_embs_dev, herb_species_ids.to(device),
            with_pairs_classes, without_pairs_classes,
            k=1
        )

        class_results_5 = evaluate_by_class_type(
            projector,
            test_embs_dev, test_species_ids.to(device),
            herb_embs_dev, herb_species_ids.to(device),
            with_pairs_classes, without_pairs_classes,
            k=5
        )

        avg_loss = running_loss / max(n_batches, 1)
        avg_acc = running_acc / max(n_batches, 1)
        current_lr = optimizer.param_groups[0]['lr']

        # Save history
        history['train_loss'].append(avg_loss)
        history['train_acc'].append(avg_acc)
        history['val_acc@1'].append(val_acc1)
        history['val_acc@5'].append(val_acc5)
        history['val_map'].append(val_map)
        history['avg_per_class_acc'].append(avg_per_class_acc)

        history['with_pairs_acc@1'].append(class_results_1['with_pairs']['acc@k'])
        history['with_pairs_acc@5'].append(class_results_5['with_pairs']['acc@k'])
        history['with_pairs_map'].append(
            class_results_5['with_pairs'].get('map', 0.0)
        )
        history['with_pairs_avg_per_class_acc'].append(class_results_1['with_pairs']['avg_per_species_acc'])

        history['without_pairs_acc@1'].append(class_results_1['without_pairs']['acc@k'])
        history['without_pairs_acc@5'].append(class_results_5['without_pairs']['acc@k'])
        history['without_pairs_map'].append(
            class_results_5['without_pairs'].get('map', 0.0)
        )
        history['without_pairs_avg_per_class_acc'].append(class_results_1['without_pairs']['avg_per_species_acc'])

        history['learning_rate'].append(current_lr)

        # ----------------------------------------------------
        # Print status
        # ----------------------------------------------------
        print(f"Epoch {epoch}:")
        print(f" Train - Loss: {avg_loss:.4f}, Acc: {avg_acc:.4f}")
        print(f" Val Overall - Acc@1: {val_acc1:.4f}, Acc@5: {val_acc5:.4f}, mAP: {val_map:.4f} ,Average Per class accuracy: {avg_per_class_acc:.4f}")

        print(f" With Pairs ({class_results_1['with_pairs']['count']} samples)"
              f" - Acc@1: {class_results_1['with_pairs']['acc@k']:.4f},"
              f" Acc@5: {class_results_5['with_pairs']['acc@k']:.4f}",
              f" Average per class accuracy: {class_results_1['with_pairs']['avg_per_species_acc']:.4f}")

        print(f" Without Pairs ({class_results_1['without_pairs']['count']} samples)"
              f" - Acc@1: {class_results_1['without_pairs']['acc@k']:.4f},"
              f" Acc@5: {class_results_5['without_pairs']['acc@k']:.4f}",
              f" Average per class accuracy: {class_results_1['without_pairs']['avg_per_species_acc']:.4f}")

        print(f" LR: {current_lr:.6f}")

        # ----------------------------------------------------
        # Early stopping logic
        # ----------------------------------------------------
        if val_map > best_map:
            best_map = val_map
            best_epoch = epoch
            early_stop_counter = 0
            best_state_dict = projector.state_dict()
            torch.save(projector.state_dict(), "best_projector.pth")
            print(f" Improved mAP → {best_map:.4f} (saving model)")
        else:
            early_stop_counter += 1
            print(f" No improvement for {early_stop_counter} epoch(s)")

        if early_stop_counter >= patience:
            print(f"\n=== EARLY STOPPING at epoch {epoch} ===")
            print(f"Restoring best model from epoch {best_epoch} (mAP {best_map:.4f})")
            projector.load_state_dict(best_state_dict)
            break

        scheduler.step()

    return projector, history, with_pairs_classes, without_pairs_classes

# ----------------------------
# UPDATED: Main pipeline
# ----------------------------

def main_pipeline_with_class_evaluation():
    print("Pipeline with Class-Type Evaluation...")
    dinov2_model = load_dinov2_from_kagglehub(device=DEVICE)

    photo_loaded = load_embeddings_from_drive("photo")
    herb_loaded = load_embeddings_from_drive("herbarium")
    test_loaded = load_embeddings_from_drive("test")
    if photo_loaded and herb_loaded and test_loaded is not None:
            photo_embs, photo_labels, photo_species_ids, meta_photo = photo_loaded
            herb_embs, herb_labels, herb_species_ids, meta_herb = herb_loaded
            test_embs, test_labels, test_species_ids, meta_test = test_loaded
    else:
            photo_embs, photo_labels, photo_species_ids, meta_photo=extract_and_save_embeddings(dinov2_model, "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset/list/train.txt", "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset", save_prefix="photo",filter_subfolder="photo",
                                species_map=None, image_size=IMAGE_SIZE,
                                batch_size=BATCH_SIZE_EXTRACT, device=DEVICE)
            herb_embs, herb_labels, herb_species_ids, meta_herb=extract_and_save_embeddings(dinov2_model, "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset/list/train.txt", "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset", save_prefix="herbarium",filter_subfolder="herbarium",
                                species_map=None, image_size=IMAGE_SIZE,
                                batch_size=BATCH_SIZE_EXTRACT, device=DEVICE)
            test_embs, test_labels, test_species_ids, meta_test =extract_and_save_embeddings(dinov2_model, "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset/list/groundtruth.txt", "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset", save_prefix="test",filter_subfolder=None,
                                species_map=None, image_size=IMAGE_SIZE,
                                batch_size=BATCH_SIZE_EXTRACT, device=DEVICE)

    # Ensure CPU
    photo_embs = photo_embs.cpu()
    herb_embs = herb_embs.cpu()
    test_embs= test_embs.cpu()
    photo_species_ids = photo_species_ids.cpu()
    herb_species_ids = herb_species_ids.cpu()
    test_species_ids = test_species_ids.cpu()

    print(f"Data loaded - Photos: {photo_embs.shape}, Herbarium: {herb_embs.shape}, Test : {test_embs.shape}")
    if test_species_ids is None:
        print("test_species_ids is NONE??")
    # Train with class-type evaluation
    projector, history, with_pairs_classes, without_pairs_classes= train_triplet_with_class_evaluation(
        photo_embs, photo_species_ids, herb_embs, herb_species_ids,test_embs,test_species_ids,
        dinov2_model,proj_dim=1024, epochs=30, batch_size=256, margin=0.3, lr=3e-4)#

    # Plot comparison
    plot_class_comparison(history, save_path=os.path.join(SAVE_DIR, "class_comparison.png"))

    return projector, history, with_pairs_classes, without_pairs_classes

projector, history, with_pairs_classes, without_pairs_classes= main_pipeline_with_class_evaluation()

"""## Confusion matrix"""

def plot_best_worst_confusion_matrices(
        projector,
        test_embs,
        test_species_ids,
        herb_embs,
        herb_species_ids,
        label_to_idx,      # {species_id → 0..C-1}
        top_k=20,
        device="cuda"
    ):
    """
    Generate:
      ✓ Full confusion matrix  (saved as cm_full.png)
      ✓ Top-20 best confusion matrix  (saved as cm_best20.png)
      ✓ Top-20 worst confusion matrix (saved as cm_worst20.png)
    """

    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from sklearn.metrics import confusion_matrix

    projector.eval()

    # ---------------------------------------------
    # 1. Project embeddings
    # ---------------------------------------------
    with torch.no_grad():
        test_proj = projector(test_embs.to(device))
        herb_proj = projector(herb_embs.to(device))

        test_proj = torch.nn.functional.normalize(test_proj, dim=1)
        herb_proj = torch.nn.functional.normalize(herb_proj, dim=1)

    # ---------------------------------------------
    # 2. NN predictions
    # ---------------------------------------------
    sim = torch.mm(test_proj, herb_proj.t())     # (N_test × N_herb)
    nn_idx = torch.argmax(sim, dim=1).cpu()      # predicted herb index

    # ---------------------------------------------
    # 3. Convert to species IDs
    # ---------------------------------------------
    y_true = test_species_ids.cpu().numpy()
    y_pred = herb_species_ids[nn_idx].cpu().numpy()

    # ---------------------------------------------
    # 4. Convert species → continuous class index
    # ---------------------------------------------
    idx_to_label = {v: k for k, v in label_to_idx.items()}
    NUM_CLASSES = len(label_to_idx)

    y_true_idx = np.array([label_to_idx[c] for c in y_true])
    y_pred_idx = np.array([label_to_idx[c] for c in y_pred])

    # ---------------------------------------------
    # 5. Full confusion matrix (normalized)
    # ---------------------------------------------
    cm = confusion_matrix(
        y_true_idx,
        y_pred_idx,
        labels=np.arange(NUM_CLASSES),
        normalize="true"
    )

    real_labels = [idx_to_label[i] for i in range(NUM_CLASSES)]

    # ---------------------------------------------
    # 6. Determine best/worst 20 classes
    # ---------------------------------------------
    per_class_acc = np.diag(cm)
    sorted_idx = np.argsort(per_class_acc)

    worst20 = sorted_idx[:top_k]
    best20 = sorted_idx[-top_k:]

    # ---------------------------------------------
    # 7. Plot + save function
    # ---------------------------------------------
    def plot_and_save(matrix, class_indices, title, filename, cmap):
        plt.figure(figsize=(14, 12), dpi=100)
        sns.heatmap(
            matrix[np.ix_(class_indices, class_indices)],
            xticklabels=[real_labels[i] for i in class_indices],
            yticklabels=[real_labels[i] for i in class_indices],
            cmap=cmap,
            annot=False
        )
        plt.title(title)
        plt.xlabel("Predicted Species ID")
        plt.ylabel("True Species ID")
        plt.tight_layout()

        # Save PNG
        plt.savefig(filename, dpi=300)
        plt.show()
        plt.close()

        print(f"Saved: {filename}")

    # ---------------------------------------------
    # 8. Generate and save all confusion matrices
    # ---------------------------------------------
    print("\n--- Saving FULL confusion matrix ---")
    plot_and_save(cm, np.arange(NUM_CLASSES), "Full Confusion Matrix", "cm_full.png", "viridis")

    print("\n--- Saving TOP 20 BEST confusion matrix ---")
    plot_and_save(cm, best20, "Top 20 Best Classes — Confusion Matrix", "cm_best20.png", "Blues")

    print("\n--- Saving TOP 20 WORST confusion matrix ---")
    plot_and_save(cm, worst20, "Top 20 Worst Classes — Confusion Matrix", "cm_worst20.png", "Reds")

    print("\nAll confusion matrices saved.")

    return cm, best20, worst20, per_class_acc

from sklearn.metrics import classification_report
import pandas as pd
import os

def generate_classification_report(test_species_ids, pred_species_ids, label_to_idx, base_dir, filename="classification_report.csv"):
    """
    Generate classification/misclassification report, show first 30 rows, and save full CSV.

    test_species_ids: true species IDs (tensor or numpy array)
    pred_species_ids: predicted species IDs (tensor or numpy array)
    label_to_idx: {species_id: class_index}
    base_dir: directory to save CSV
    filename: CSV filename
    """
    # Ensure numpy arrays
    y_true_all = test_species_ids.cpu().numpy() if torch.is_tensor(test_species_ids) else np.array(test_species_ids)
    y_pred_all = pred_species_ids.cpu().numpy() if torch.is_tensor(pred_species_ids) else np.array(pred_species_ids)

    # Map class indices back to real species IDs for readable report
    idx_to_label = {v: k for k, v in label_to_idx.items()}

    y_true_labels = [idx_to_label[label_to_idx[i]] if i in label_to_idx else i for i in y_true_all]
    y_pred_labels = [idx_to_label[label_to_idx[i]] if i in label_to_idx else i for i in y_pred_all]

    # Generate report
    report = classification_report(
        y_true_labels,
        y_pred_labels,
        digits=3,
        output_dict=True,
        zero_division=0
    )

    report_df = pd.DataFrame(report).transpose()

    print("✅ Misclassification Report (first 30 rows):")
    display(report_df.head(30))

    # Save full report
    os.makedirs(base_dir, exist_ok=True)
    report_path = os.path.join(base_dir, filename)
    report_df.to_csv(report_path)
    print(f"✅ Saved full report to: {report_path}")

    return report_df

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
def main_pipeline_with_confusion_matrix():
    print("Pipeline with Class-Type Evaluation...")
    dinov2_model = load_dinov2_from_kagglehub(device=DEVICE)

    photo_loaded = load_embeddings_from_drive("photo")
    herb_loaded = load_embeddings_from_drive("herbarium")
    test_loaded = load_embeddings_from_drive("test")
    if photo_loaded and herb_loaded and test_loaded is not None:
            photo_embs, photo_labels, photo_species_ids, meta_photo = photo_loaded
            herb_embs, herb_labels, herb_species_ids, meta_herb = herb_loaded
            test_embs, test_labels, test_species_ids, meta_test = test_loaded
    else:
            photo_embs, photo_labels, photo_species_ids, meta_photo=extract_and_save_embeddings(dinov2_model, "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset/list/train.txt", "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset", save_prefix="photo",filter_subfolder="photo",
                                species_map=None, image_size=IMAGE_SIZE,
                                batch_size=BATCH_SIZE_EXTRACT, device=DEVICE)
            herb_embs, herb_labels, herb_species_ids, meta_herb=extract_and_save_embeddings(dinov2_model, "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset/list/train.txt", "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset", save_prefix="herbarium",filter_subfolder="herbarium",
                                species_map=None, image_size=IMAGE_SIZE,
                                batch_size=BATCH_SIZE_EXTRACT, device=DEVICE)
            test_embs, test_labels, test_species_ids, meta_test =extract_and_save_embeddings(dinov2_model, "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset/list/groundtruth.txt", "/kaggle/input/aml-herbarium-field/AML_project_herbarium_dataset", save_prefix="test",filter_subfolder=None,
                                species_map=None, image_size=IMAGE_SIZE,
                                batch_size=BATCH_SIZE_EXTRACT, device=DEVICE)

    # Ensure CPU
    photo_embs = photo_embs.cpu()
    herb_embs = herb_embs.cpu()
    test_embs= test_embs.cpu()
    photo_species_ids = photo_species_ids.cpu()
    herb_species_ids = herb_species_ids.cpu()
    test_species_ids = test_species_ids.cpu()

    input_dim = photo_embs.shape[1]
    projector = SharedProjector(input_dim, proj_dim=1024).to(DEVICE)
    ckpt = torch.load("/kaggle/working/best_projector.pth", map_location=DEVICE)
    projector.load_state_dict(ckpt)




    all_species_ids = torch.cat([herb_species_ids, photo_species_ids]).unique().tolist()

    # Sort to ensure consistent ordering
    all_species_ids = sorted([int(s) for s in all_species_ids])

    # Build label_to_idx
    label_to_idx = {species_id: idx for idx, species_id in enumerate(all_species_ids)}

    # Optional reverse map
    idx_to_label = {idx: species_id for species_id, idx in label_to_idx.items()}


    projector.eval()
    with torch.no_grad():
        test_proj = projector(test_embs.to(DEVICE))
        herb_proj = projector(herb_embs.to(DEVICE))

        test_proj = torch.nn.functional.normalize(test_proj, dim=1)
        herb_proj = torch.nn.functional.normalize(herb_proj, dim=1)

        sim = torch.mm(test_proj, herb_proj.t())       # (N_test x N_herb)
        nn_idx = torch.argmax(sim, dim=1).cpu()       # top-1 predicted herb index

        y_true = test_species_ids.cpu().numpy()       # ground truth
        y_pred = herb_species_ids[nn_idx].cpu().numpy()  # predicted species

    cm, best20, worst20, per_class_acc = plot_best_worst_confusion_matrices(
        projector,
        test_embs,
        test_species_ids,
        herb_embs,
        herb_species_ids,
        label_to_idx,     # must be provided
        top_k=20,
        device=DEVICE
    )


    report_df = generate_classification_report(
        test_species_ids=y_true,
        pred_species_ids=y_pred,
        label_to_idx=label_to_idx,
        base_dir="./confusion_matrices",
        filename="full_classification_report.csv"
    )

    print(f"Data loaded - Photos: {photo_embs.shape}, Herbarium: {herb_embs.shape}, Test : {test_embs.shape}")

main_pipeline_with_confusion_matrix()