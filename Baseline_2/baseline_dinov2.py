# -*- coding: utf-8 -*-
"""baseline_dinov2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dZYPC5Tw1WlwjwuTuB9BeokXmvXgZAnr

Benjamin Hii 102785859

# Cross Domain Plant Species Identitication

To Upload and Host on Google Cloud, just upload all of the project folder (except "pretrained_model") onto "Ben" folder on "AML_Project" project folder.

## Import
"""

import torch
import torch.nn as nn
import os

# ViT (dinov2) Builder
import timm

# Mount Google Drive inside Notebook

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/"

import sys

project_root = "/content/drive/MyDrive/AML_Group_Project/Approach_2/Ben"
sys.path.append(project_root)

if os.path.exists(project_root):
    print("✅ Path exists: ", project_root)
else:
    print("❌ Path does NOT exist: ", project_root)

"""#### Check Torch version"""

print(f'Pytorch version: {torch.__version__}')
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)
try:
    print(f'CUDA version: {torch.version.cuda}')
except:
    pass

if torch.cuda.is_available():
    print("Current GPU:", torch.cuda.current_device())
    print("GPU name:", torch.cuda.get_device_name(torch.cuda.current_device()))
    print("CUDA version (PyTorch built with):", torch.version.cuda)
else:
    print("CUDA Unavailable.")

"""## DinoV2 Model

#### Download Model
"""

# # Do not run after running the first time

# import kagglehub
# import shutil
# import os

# # Model Download Location
# dst_path = "../models/dinov2"

# # Check if the model already exists
# if os.path.exists(dst_path) and os.listdir(dst_path):
#     print(f"Model already exists at: {dst_path}. Skipping download.")
# else:
#     # Download model from Kaggle
#     src_path = kagglehub.model_download(
#         "juliostat/dinov2_patch14_reg4_onlyclassifier_then_all/pyTorch/default"
#     )
#     print("KaggleHub cache:", src_path)

#     os.makedirs(dst_path, exist_ok=True)

#     # Move Model Files
#     for fname in os.listdir(src_path):
#         shutil.copy(os.path.join(src_path, fname), dst_path)

#     print("Model copied to:", dst_path)

# print("Files:", os.listdir(dst_path))

"""#### Import Model by Loading Checkpoint"""

import argparse
from torch.serialization import add_safe_globals

# Allowlist argparse.Namespace for safe load
add_safe_globals([argparse.Namespace])

# Load Checkpoint
checkpoint = torch.load(
    f"{project_root}/pretrained_models/dinov2/model_best.pth.tar",
    map_location="cpu",
    weights_only=False
)

print("Checkpoint loaded successfully ✅")
print(checkpoint.keys())

"""#### Initiate & Load Model Weights onto DinoV2 Structure"""

# Initiate DinoV2 structure
backbone = timm.create_model("vit_base_patch14_reg4_dinov2", pretrained=False)

# Load checkpoint into DinoV2
state_dict = checkpoint["state_dict"]
state_dict = {k: v for k, v in state_dict.items() if not k.startswith("head.")}
backbone.load_state_dict(state_dict, strict=True)
backbone = backbone.to(device)

# Set to evaluation mode
backbone.eval

print(f"✓ Model is on: {next(backbone.parameters()).device}")

"""#### Freezing DinoV2 Blocks"""

# Stop changes / Freezing DinoV2
for param in backbone.parameters():
    param.requires_grad = False

"""#### Loading Dataset from Google Drive"""

dataloader_path = os.path.join(project_root, "src", "dataloader.py")
print(f"dataloader.py exists: {os.path.exists(dataloader_path)}")

from src.dataloader import data_loaders

# Set your dataset folder path
dataset_root = "/content/drive/MyDrive/AML_Group_Project/dataset"

if os.path.exists(dataset_root):
    print("✅ Path exists: ", dataset_root)
else:
    print("❌ Path does NOT exist: ", dataset_root)

# Locate Training & Testing CSV
train_list = f"{dataset_root}/list/train.txt"
test_list = f"{dataset_root}/list/test.txt"
groundtruth_file = f"{dataset_root}/list/groundtruth.txt"
species_list = f"{dataset_root}/list/species_list.txt"

# Load data loaders
train_loader, val_loader = data_loaders(
    train_list=train_list,
    test_list=test_list,
    batch_size=32,
    image_size=518,
    dataset_root=dataset_root,
    groundtruth_file=groundtruth_file
)

# Print the number of batches
print(f"Number of training batches: {len(train_loader)}")
print(f"Number of testing batches: {len(val_loader)}")

# Print the total number of images for both
print(f"Total training images: {len(train_loader.dataset)}")
print(f"Total validation images: {len(val_loader.dataset)}")

"""#### Feature Embedding (Feature Extraction)"""

# Import feature extraction from py file
from src.feature_extractor import extract_features

embedding_path = f"{project_root}/embeddings"

"""###### Extract Training Features"""

# print(f"\nExtracting Training Dataset Features")

# train_features, train_labels = extract_features(backbone, train_loader, device)

# print(f"\nTraining features extracted:")
# print(f"  Shape: {train_features.shape}")
# print(f"  Type: {train_features.dtype}")
# print(f"  Device: {train_features.device}")
# print(f"\nTraining labels:")
# print(f"  Shape: {train_labels.shape}")
# print(f"  Unique classes: {len(torch.unique(train_labels))}")

# # Save training feature
# torch.save({
#     'features': train_features,
#     'labels': train_labels,
#     'num_samples': len(train_features),
#     'feature_dim': train_features.shape[1]
# }, os.path.join(embedding_path, 'train_features.pt'))

"""###### Extract Validation Features"""

# print(f"\nExtracting Validation Dataset Features")

# val_features, val_labels = extract_features(backbone, val_loader, device)

# print(f"\nValidation features extracted:")
# print(f"  Shape: {val_features.shape}")
# print(f"  Type: {val_features.dtype}")
# print(f"  Device: {val_features.device}")
# print(f"\nValidation labels:")
# print(f"  Shape: {val_labels.shape}")
# print(f"  Unique classes: {len(torch.unique(val_labels))}")

# # Save validation features
# torch.save({
#     'features': val_features,
#     'labels': val_labels,
#     'num_samples': len(val_features),
#     'feature_dim': val_features.shape[1]
# },  os.path.join(embedding_path, 'val_features.pt'))

"""#### Loading Extracted Features"""

# Use this cell to load extracted features instead of needing to extract & embed again

# Load training features
train_data = torch.load(os.path.join(embedding_path, 'train_features.pt'))
train_features = train_data['features']
train_labels = train_data['labels']

# Load validation features
val_data = torch.load(os.path.join(embedding_path, 'val_features.pt'))
val_features = val_data['features']
val_labels = val_data['labels']

print("✓ Features loaded from disk!")
print(f"Train features: {train_features.shape}")
print(f"Val features: {val_features.shape}")

"""## Feature Classification Head Section"""

# More Imports
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tqdm import tqdm

# Import classifier models
from classifier_head.linear_classifier import LinearClassifier
from classifier_head.random_forest_classifier import RandomForestClassifier
from classifier_head.logistic_regression_classifier import LogisticRegressionClassifier
from classifier_head.svm_rbf_classifier import SVM_RBF_Classifier
from classifier_head.MLP_classifier import MLPClassifier
from classifier_head.improved_MLP_classifier import ImprovedMLPClassifier
from classifier_head.residual_classifier import ResidualMLPClassifier
from classifier_head.efficientNet_classifier import EfficientNetClassifier
from classifier_head.deep_efficientNet import EfficientNetStyleHead_Deep

"""## Model Training"""

# Import training function
from src.train_classifier import train_classifier

# Remap labels to [0, num_classes-1]
unique_labels = np.unique(np.concatenate([train_labels.numpy(), val_labels.numpy()]))
num_classes = len(unique_labels)
label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}

print(f"Number of classes: {num_classes}")
print(f"Original label range: [{unique_labels.min()}, {unique_labels.max()}]")

# Remap
train_labels_remapped = torch.tensor([label_to_idx[label.item()] for label in train_labels])
val_labels_remapped = torch.tensor([label_to_idx[label.item()] for label in val_labels])

print(f"Remapped label range: [{train_labels_remapped.min()}, {train_labels_remapped.max()}]")

"""## Different Classifier Heads

#### Traditional Classifier

#### Logistic Regression Classifier Head
"""

# model = LogisticRegressionClassifier(input_dim=768, num_classes=num_classes, C=0.5)

"""#### SVM (Non-linear) Classifier Head"""

# model = SVM_RBF_Classifier(
#     input_dim=768,
#     num_classes=num_classes,
#     C=0.1,           # Lower C = more regularization (try 1.0, 5.0, 10.0, 50.0)
#     gamma='scale',   # Or try 'auto' or a specific float like 0.001
#     class_weight='balanced'
# )

# Quick test different regularization strengths for SVM RBF
# for C_val in [0.01, 0.05, 0.1, 0.5, 1.0]:
#     print(f"\n{'='*70}")
#     print(f"Testing SVM RBF with C={C_val}")
#     print(f"{'='*70}")

#     model = SVM_RBF_Classifier(
#         input_dim=768,
#         num_classes=num_classes,
#         C=C_val,
#         gamma='scale',
#         class_weight='balanced'
#     )

#     trained_model, history = train_classifier(
#         model=model,
#         train_features=train_features,
#         train_labels=train_labels_remapped,
#         val_features=val_features,
#         val_labels=val_labels_remapped,
#         device=device
#     )

#     print(f"C={C_val}: Train={history['train_acc'][0]:.2f}%, Val={history['val_acc'][0]:.2f}%")

"""#### Random Forest Classifier Head"""

# model = RandomForestClassifier(
#     input_dim=768,
#     num_classes=num_classes,
#     n_estimators=1000,
#     max_depth=40,
#     min_samples_split=5,
#     min_samples_leaf=2
# )

"""#### Simple Linear Classifier Head"""

model = LinearClassifier(input_dim=768, num_classes=num_classes)

"""####  2-Layer MLP + Dropout Classifier Head"""

# model = MLPClassifier(input_dim=768, hidden_dim=256, num_classes=num_classes, dropout=0.4)

"""####  Improved MLP Head"""

# model = ImprovedMLPClassifier(
#     input_dim=768,
#     hidden_dim=512,
#     num_classes=num_classes,
#     dropout=0.4
# )

"""#### Residual MLP Classifier Head"""

# model = ResidualMLPClassifier(input_dim=768, hidden_dim=256, num_classes=num_classes,
#                                 num_blocks=1, dropout=0.4)

"""#### EfficientNet Style Classifier Head"""

# model = EfficientNetClassifier(
#     input_dim=768,
#     num_classes=num_classes,
#     dropout=0.4
# ).to(device)

# model = EfficientNetStyleHead_Deep(
#     input_dim=768,
#     num_classes=num_classes,
#     dropout=0.4
# ).to(device)

"""#### Train Model"""

trained_model, history = train_classifier(
    model=model,
    train_features=train_features,
    train_labels=train_labels_remapped,
    val_features=val_features,
    val_labels=val_labels_remapped,
    num_epochs=300,
    batch_size=128,
    learning_rate=0.0001,
    weight_decay=5e-4,
    label_smoothing=0.0,
    add_noise=False,
    early_stop_patience=100,
    use_cosine_schedule=True,
    device=device
)

print("\n✓ Training complete")

training_params = {
    'input_dim': 768,
    'hidden_dim': 512,
    'num_classes': num_classes,
    'dropout': 0.4,
    'num_epochs': 150,
    'batch_size': 64,
    'learning_rate': 0.001,
    'weight_decay': 1e-3,
    'optimizer': 'AdamW',
    'label_smoothing': 0.1,
    'add_noise': True,
    'noise_std': 0.1,
    'early_stop_patience': 20,
    'model_type': 'Classifier'
}

"""## Result Plotting & Visualisation"""

# Import evaluation function
from src.evaluation import evaluate_classifier
from src.topk_accuracy import calculate_topk_accuracy
from src.truth_matrix import plot_confusion_matrix
from src.plot_training import plot_training_history
from src.save_results import save_training_results

# Save results path
result_path = f"{project_root}/results"

# Evaluate on validation set
val_accuracy, val_predictions = evaluate_classifier(
    model=trained_model,
    features=val_features,
    labels=val_labels_remapped,
    device=device
)

print(f"Validation Accuracy: {val_accuracy*100:.2f}%")

# Plot Training & Validation Graph
plot_training_history(history, save_path=f"{result_path}/training_curves.png")

"""## Unpaired / Paired Results"""

from src.paired_unpaired_eval import test_paired_unpaired_split, print_detailed_results

paired_unpaired_results = test_paired_unpaired_split(
    model=trained_model,
    val_features=val_features,
    val_labels=val_labels_remapped,
    test_list_file=test_list,
    dataset_root=dataset_root,
    label_to_idx=label_to_idx,
    device=device
)

print_detailed_results(paired_unpaired_results)

"""## Top-k Accuracy"""

# Calculate Top-K accuracy
topk_results = calculate_topk_accuracy(model, val_features, val_labels_remapped, device)
print(f"Top-1: {topk_results['top1']:.2f}%")
print(f"Top-5: {topk_results['top5']:.2f}%")

save_training_results(
    history=history,
    val_accuracy=val_accuracy,
    topk_results=topk_results,
    model_name="classifier",  # Change based on which type of the model that is used
    paired_unpaired_results=paired_unpaired_results,
    save_dir=result_path,
    model=trained_model,
    training_params = training_params
)

# Create confusion matrix
plot_confusion_matrix(val_predictions, val_labels_remapped.numpy(), save_path=f"{result_path}/confusion_matrix.png")

"""## Saving Model Created"""

# Import Save model function
from src.model_utils import save_best_model_from_training

# Save the trained model
model_save_dir = f"{project_root}/created_models"
save_path = save_best_model_from_training(
    model=trained_model,
    history=history,
    save_dir=model_save_dir,
    model_name="linear_classifier"  # Change to depending on classifier used
)

# Break point

"""## Bottleneck / Statistic Check"""

import torch
# import numpy as np

# print("="*70)
# print("BOTTLENECK DIAGNOSTIC")
# print("="*70)

# # 1. Check training/validation split
# print(f"\n1. DATA SPLIT:")
# print(f"   Training samples: {len(train_features)}")
# print(f"   Validation samples: {len(val_features)}")
# print(f"   Number of classes: {num_classes}")
# print(f"   Samples per class (train): {len(train_features) / num_classes:.1f}")
# print(f"   Samples per class (val): {len(val_features) / num_classes:.1f}")

# if len(train_features) / num_classes < 50:
#     print("   ⚠️  WARNING: Very few samples per class (<50)")
#     print("   → This limits maximum achievable accuracy")

# # 2. Check label distribution
# print(f"\n2. LABEL DISTRIBUTION:")
# train_label_counts = torch.bincount(train_labels_remapped)
# val_label_counts = torch.bincount(val_labels_remapped)

# print(f"   Train class imbalance ratio: {train_label_counts.max().item() / train_label_counts.min().item():.2f}:1")
# print(f"   Val class imbalance ratio: {val_label_counts.max().item() / val_label_counts.min().item():.2f}:1")

# if train_label_counts.max().item() / train_label_counts.min().item() > 10:
#     print("   ⚠️  WARNING: Severe class imbalance (>10:1)")
#     print("   → Model biased toward majority classes")

# # 3. Check feature quality
# print(f"\n3. FEATURE QUALITY:")
# print(f"   Feature dimension: {train_features.shape[1]}")
# print(f"   Feature mean: {train_features.mean():.4f}")
# print(f"   Feature std: {train_features.std():.4f}")
# print(f"   Feature min: {train_features.min():.4f}")
# print(f"   Feature max: {train_features.max():.4f}")

# # Check for NaN or Inf
# nan_count = torch.isnan(train_features).sum().item()
# inf_count = torch.isinf(train_features).sum().item()
# print(f"   NaN values: {nan_count}")
# print(f"   Inf values: {inf_count}")

# if nan_count > 0 or inf_count > 0:
#     print("   ❌ ERROR: Features contain NaN or Inf!")

# # 4. Check feature separability
# print(f"\n4. FEATURE SEPARABILITY:")
# from sklearn.decomposition import PCA
# from sklearn.manifold import TSNE

# # Quick PCA check
# pca = PCA(n_components=2)
# train_pca = pca.fit_transform(train_features.numpy()[:1000])  # Sample for speed
# explained_var = pca.explained_variance_ratio_.sum()

# print(f"   PCA (2 components) explains: {explained_var*100:.1f}% variance")

# if explained_var < 0.2:
#     print("   ⚠️  WARNING: Low variance in top 2 PCs (<20%)")
#     print("   → Features may not be very discriminative")

# # 5. Check train-val distribution shift
# print(f"\n5. DOMAIN SHIFT CHECK:")
# train_mean = train_features.mean(dim=0)
# val_mean = val_features.mean(dim=0)
# distribution_diff = (train_mean - val_mean).abs().mean().item()

# print(f"   Mean feature difference (train vs val): {distribution_diff:.4f}")

# if distribution_diff > 0.5:
#     print("   ⚠️  WARNING: Large distribution shift (>0.5)")
#     print("   → Train and val are from different distributions (domain gap)")

# # 6. Check overfitting
# print(f"\n6. OVERFITTING CHECK:")
# if 'history' in globals():
#     final_train_acc = history['train_acc'][-1]
#     final_val_acc = history['val_acc'][-1]
#     gap = final_train_acc - final_val_acc

#     print(f"   Final train accuracy: {final_train_acc:.2f}%")
#     print(f"   Final val accuracy: {final_val_acc:.2f}%")
#     print(f"   Overfitting gap: {gap:.2f}%")

#     if gap > 15:
#         print("   ⚠️  WARNING: Severe overfitting (gap >15%)")
#         print("   → Model memorizing training data")
#     elif gap < 3:
#         print("   ⚠️  WARNING: Possible underfitting (gap <3%)")
#         print("   → Model not learning enough")

# # 7. Check class confusion
# print(f"\n7. PER-CLASS PERFORMANCE:")
# if 'trained_model' in globals():
#     trained_model.eval()
#     with torch.no_grad():
#         outputs = trained_model(val_features.to(device))
#         _, predictions = outputs.max(1)

#     predictions = predictions.cpu()

#     # Find worst classes
#     per_class_acc = []
#     for c in range(num_classes):
#         mask = val_labels_remapped == c
#         if mask.sum() > 0:
#             acc = (predictions[mask] == val_labels_remapped[mask]).float().mean().item()
#             per_class_acc.append((c, acc, mask.sum().item()))

#     per_class_acc.sort(key=lambda x: x[1])

#     print(f"   Worst 5 classes:")
#     for i, (cls, acc, count) in enumerate(per_class_acc[:5]):
#         print(f"     Class {cls}: {acc*100:.1f}% ({count} samples)")

#     print(f"   Best 5 classes:")
#     for i, (cls, acc, count) in enumerate(per_class_acc[-5:]):
#         print(f"     Class {cls}: {acc*100:.1f}% ({count} samples)")

#     avg_acc = np.mean([x[1] for x in per_class_acc])
#     print(f"   Average per-class accuracy: {avg_acc*100:.1f}%")

# print("\n" + "="*70)

